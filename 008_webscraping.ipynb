{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e939293c-0315-47d0-8346-28c98c495fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "link = \"https://scriptureearth.org/data/bom/sab/bom/bom-22-SNG-001.html\"\n",
    "req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "url = urlopen(req).read()\n",
    "soup = BeautifulSoup(url, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "640f1566-f9cd-4508-a7d2-323960627adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lwɛlɛ_Sɔlɔmɔn_1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_chpt = soup.find('title').text.replace(\" \", \"_\")\n",
    "# b_chpt = b_chpt.replace(\" \", \"_\")\n",
    "b_chpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31878499-1688-4a2b-ad01-47a720698fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Lwɛlɛ̂ Sɔlɔmɔn de gwa dal lɛlɛ mwa na neta,\n",
      "\n",
      "\n",
      "2 Ka hwà yang man ranang na me!\n",
      "Yaga tɛ̂ yey mo ha dal na neta e ra nshî anap.\n",
      "3 Něy mɔmɔ mo ne ro nesang hyɔɔlɔl,\n",
      "wôk reza mo zɔng ha sé rat sede wok nesang něy.\n",
      "Yaga ano de beha mwa be tɛ̀ yey na hwo.\n",
      "4 Jut me, na wot a nára hot!\n",
      "À gbɔng gwɔ̀m hom, yel na me e duk pɔlɔ mō,\n",
      "tik na wot vɔk yey pyɛng na wot ra nzem tɛ̂ yey mo,\n",
      "wot ê somo tɛ̂ yey mo hɛ dal nshî anap.\n",
      "A sé tyɛng be tɛ̀ yey na hwo.\n",
      "5 À behwong Yɛrusalɛm, ma sé nèros bes, ko kasâng-hwong,\n",
      "nèros sede bûk hey e Kedar,\n",
      "ko zɛrɛ sede berugû tɔrɔ̂ lɔ Gbɔng Gwɔ̀m Sɔlɔmɔn.\n",
      "6 Yin lòlo bayis e ra me yaga ma sé nèros wɛt,\n",
      "yaga e gwi gwɛ gwa tɛ̀ me ano.\n",
      "Begwa hom ba vɔk shom na me,\n",
      "ba tɛ̀ me mwât bɛrɛng bě anap,\n",
      "ko ma bɛrɛng to hom wɛt, sede bwi de ba hala gwɛ.\n",
      "7 À wò yey hom, ha a me,\n",
      "kwɔn de hwó gyenges ɛ vyēl mo,\n",
      "na kwɔn de hwó tik yɛ ye dyɛng ɛ e necam nagwi.\n",
      "Ano mê kyě rɔ̀nɔs jáma mo e kara mó vè behak wɛt.\n",
      "\n",
      "8 Ka hwà tɔk wɛt, à hwo de hwà dal sé zɛrɛ e ji behwong,\n",
      "raa nekyɔ̌n kwarâ vyēl mo,\n",
      "hwô sɛ̀ a nèy vyēl mo kwɔn de hwô gyeng ɛ yɛ\n",
      "bagat na bǔk bemât gyeng vyēl.\n",
      "9 À sa hom, ma tyɛngrɛ hwo\n",
      "sede gwining e ji bedwa de ba tɔrɔ na nzem yaga hwɔlɔ̂ kyɛkyɛ̂ Farawo.\n",
      "10 Rànang mo ba sé rat na yɔ̄bɔ betong,\n",
      "fwɔ mo gwa sé rwat na lɛlɛ̂ fwɔ.\n",
      "11 Tik na be yóng a hwo nerwěy ne dyam shinang,\n",
      "na be dak ɛ dyam pyɛng.\n",
      "\n",
      "12 Jeng de gbɔng gwɔ̀m a sé e dyɛngɛ e ra gwat mɛ,\n",
      "e sɔ nesang hyɔɔlɔl něy nât nerat e rebek hom.\n",
      "13 Wò yey hom e dyɛngɛ e tê bavasal hom,\n",
      "sede radîng něy mir.\n",
      "14 Wò yey hom a sé sede gushuk bejamûk hey,\n",
      "de ba ga tabas e bwî anap Ɛn-Gɛdi.\n",
      "\n",
      "15 À sa hom, hwà sé zɛrɛ,\n",
      "hwà sé zɛrɛ e ryat wɛt,\n",
      "bayis mo ba vɔk nehywɛ sede bebɔp.\n",
      "\n",
      "16 À wò yey hom, hwà sé zɛrɛ, kasâng-wen,\n",
      "tik wot a jut sɔng lwâ hey sede gwat mot,\n",
      "17 cɔ́gɔt sida sede sesêk lɔ mot,\n",
      "cɔ́gɔt fir sede yê kak o arɔng,\n"
     ]
    }
   ],
   "source": [
    "# Find the div element with class \"ms\"\n",
    "div_tags = soup.find('div', class_='ms')  \n",
    "\n",
    "if div_tags:\n",
    "    div_tags.extract()\n",
    "    \n",
    "# Find the div element with class \"mt\"\n",
    "div_tags = soup.find('div', class_='mt')  \n",
    "\n",
    "if div_tags:\n",
    "    div_tags.extract()\n",
    "\n",
    "# Find all div tags with class \"sp\"\n",
    "div_tags = soup.find_all('div', class_='sp') \n",
    "\n",
    "for div_tag in div_tags:\n",
    "    div_tag.extract()\n",
    "\n",
    "\n",
    "parent_div = soup.find('div', class_='m') \n",
    "\n",
    "if parent_div:\n",
    "    subdivision_div = parent_div.find('div', class_='c-drop')  \n",
    "    if subdivision_div:\n",
    "        subdivision_div.extract()\n",
    "# Print the remaining content\n",
    "text = soup.text\n",
    "\n",
    "# Remove the last part after the copyright symbol\n",
    "text = text.split('©')[0]\n",
    "\n",
    "# Regex pattern to match the specific setup\n",
    "text = text.split('>')[-1]\n",
    "text = text.strip()\n",
    "\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d839d85-efb5-4159-8c89-7e9e6fdddcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to data/Lwɛlɛ_Sɔlɔmɔn_1.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'data'  # Specify the folder path\n",
    "txt_name = f'{b_chpt}.txt'  # Specify the filename\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct the file path\n",
    "txt_path = Path(folder_path) / txt_name\n",
    "\n",
    "# Save the text to the file\n",
    "with open(txt_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(text)\n",
    "\n",
    "print(f'Text saved to {txt_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8380b439-a06d-4040-a439-52e810190c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data saved to: data/Lwɛlɛ_Sɔlɔmɔn_1.json\n"
     ]
    }
   ],
   "source": [
    "json_name = f'{b_chpt}.json'   # Specify the filename\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Construct the file path\n",
    "json_path = Path(folder_path) / json_name\n",
    "\n",
    "data = {}\n",
    "matches = re.findall(r'(\\d+)\\s+([\\s\\S]*?)(?=\\d|$)', text)\n",
    "for match in matches:\n",
    "    key = match[0]\n",
    "    value = match[1].strip()\n",
    "    data[key] = value\n",
    "\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "print(\"JSON data saved to:\", json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bdb9b-d388-47be-a26b-154fe0dd110c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
